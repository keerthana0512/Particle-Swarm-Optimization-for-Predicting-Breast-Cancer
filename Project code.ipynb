{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c885174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import os\n",
    "# initialize the path to the *original* input directory of images\n",
    "ORIG_INPUT_DATASET = \"histopathology dataset\"\n",
    "# initialize the base path to the *new* directory that will contain\n",
    "# our images after computing the training and testing split\n",
    "BASE_PATH = \"split data\"\n",
    "# derive the training, validation, and testing directories\n",
    "TRAIN_PATH = os.path.sep.join([BASE_PATH, \"training\"])\n",
    "VAL_PATH = os.path.sep.join([BASE_PATH, \"validation\"])\n",
    "TEST_PATH = os.path.sep.join([BASE_PATH, \"testing\"])\n",
    "# define the amount of data that will be used training\n",
    "TRAIN_SPLIT = 0.8\n",
    "# the amount of validation data will be a percentage of the\n",
    "# *training* data\n",
    "VAL_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import paths\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "# grab the paths to all input images in the original input directory\n",
    "# and shuffle them\n",
    "imagePaths = list(paths.list_images(ORIG_INPUT_DATASET))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "# compute the training and testing split\n",
    "i = int(len(imagePaths) * TRAIN_SPLIT)\n",
    "trainPaths = imagePaths[:i]\n",
    "testPaths = imagePaths[i:]\n",
    "# we'll be using part of the training data for validation\n",
    "i = int(len(trainPaths) * VAL_SPLIT)\n",
    "valPaths = trainPaths[:i]\n",
    "trainPaths = trainPaths[i:]\n",
    "# define the datasets that we'll be building\n",
    "datasets = [\n",
    "\t(\"training\", trainPaths, TRAIN_PATH),\n",
    "\t(\"validation\", valPaths, VAL_PATH),\n",
    "\t(\"testing\", testPaths, TEST_PATH)\n",
    "]\n",
    "# loop over the datasets\n",
    "for (dType, imagePaths, baseOutput) in datasets:\n",
    "\t# show which data split we are creating\n",
    "\tprint(\"[INFO] building '{}' split\".format(dType))\n",
    "\t# if the output base output directory does not exist, create it\n",
    "\tif not os.path.exists(baseOutput):\n",
    "\t\tprint(\"[INFO] 'creating {}' directory\".format(baseOutput))\n",
    "\t\tos.makedirs(baseOutput)\n",
    "\t# loop over the input image paths\n",
    "\tfor inputPath in imagePaths:\n",
    "\t\t# extract the filename of the input image and extract the\n",
    "\t\t# class label (\"0\" for \"negative\" and \"1\" for \"positive\")\n",
    "\t\tfilename = inputPath.split(os.path.sep)[-1]\n",
    "\t\tlabel = filename[-5:-4]\n",
    "\t\t# build the path to the label directory\n",
    "\t\tlabelPath = os.path.sep.join([baseOutput, label])\n",
    "\t\t# if the label output directory does not exist, create it\n",
    "\t\tif not os.path.exists(labelPath):\n",
    "\t\t\tprint(\"[INFO] 'creating {}' directory\".format(labelPath))\n",
    "\t\t\tos.makedirs(labelPath)\n",
    "\t\t# construct the path to the destination image and then copy\n",
    "\t\t# the image itself\n",
    "\t\tp = os.path.sep.join([labelPath, filename])\n",
    "\t\tshutil.copy2(inputPath, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7eb5b",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f19831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import os\n",
    "# initialize the base path to the *new* directory that will contain\n",
    "# our images after computing the training and testing split\n",
    "BASE_PATH = \"split data\"\n",
    "# define the names of the training, testing, and validation\n",
    "# directories\n",
    "TRAIN = \"training\"\n",
    "TEST = \"testing\"\n",
    "VAL = \"validation\"\n",
    "# initialize the list of class label names\n",
    "CLASSES = [\"0\", \"1\"]\n",
    "# set the batch size'''\n",
    "BATCH_SIZE = 32\n",
    "# initialize the label encoder file path and the output directory to\n",
    "# where the extracted features (in CSV file format) will be stored\n",
    "LE_PATH = os.path.sep.join([\"split csv\", \"le.cpickle\"])\n",
    "BASE_CSV_PATH = \"split csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2fe60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''# import the necessary packages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "# load the ResNet50 network and initialize the label encoder\n",
    "print(\"[INFO] loading network...\")\n",
    "model = ResNet50(weights=\"imagenet\", include_top=False)\n",
    "le = None\n",
    "# loop over the data splits\n",
    "for split in (TRAIN, TEST, VAL):\n",
    "\t# grab all image paths in the current split\n",
    "\tprint(\"[INFO] processing '{} split'...\".format(split))\n",
    "\tp = os.path.sep.join([BASE_PATH, split])\n",
    "\timagePaths = list(paths.list_images(p))\n",
    "\t# randomly shuffle the image paths and then extract the class\n",
    "\t# labels from the file paths\n",
    "\trandom.shuffle(imagePaths)\n",
    "\tlabels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "\t# if the label encoder is None, create it\n",
    "\tif le is None:\n",
    "\t\tle = LabelEncoder()\n",
    "\t\tle.fit(labels)\n",
    "\t# open the output CSV file for writing\n",
    "\tcsvPath = os.path.sep.join([BASE_CSV_PATH,\n",
    "\t\t\"{}.csv\".format(split)])\n",
    "\tcsv = open(csvPath, \"w\")\n",
    "    # loop over the images in batches\n",
    "\tfor (b, i) in enumerate(range(0, len(imagePaths), BATCH_SIZE)):\n",
    "\t\t# extract the batch of images and labels, then initialize the\n",
    "\t\t# list of actual images that will be passed through the network\n",
    "\t\t# for feature extraction\n",
    "\t\tprint(\"[INFO] processing batch {}/{}\".format(b + 1,\n",
    "\t\t\tint(np.ceil(len(imagePaths) / float(BATCH_SIZE)))))\n",
    "\t\tbatchPaths = imagePaths[i:i + BATCH_SIZE]\n",
    "\t\tbatchLabels = le.transform(labels[i:i + BATCH_SIZE])\n",
    "\t\tbatchImages = []\n",
    "\t\t# loop over the images and labels in the current batch\n",
    "\t\tfor imagePath in batchPaths:\n",
    "\t\t\t# load the input image using the Keras helper utility\n",
    "\t\t\t# while ensuring the image is resized to 224x224 pixels\n",
    "\t\t\timage = load_img(imagePath, target_size=(224, 224))\n",
    "\t\t\timage = img_to_array(image)\n",
    "\t\t\t# preprocess the image by (1) expanding the dimensions and\n",
    "\t\t\t# (2) subtracting the mean RGB pixel intensity from the\n",
    "\t\t\t# ImageNet dataset\n",
    "\t\t\timage = np.expand_dims(image, axis=0)\n",
    "\t\t\timage = preprocess_input(image)\n",
    "\t\t\t# add the image to the batch\n",
    "\t\t\tbatchImages.append(image)\n",
    "            \t# pass the images through the network and use the outputs as\n",
    "\t\t# our actual features, then reshape the features into a\n",
    "\t\t# flattened volume\n",
    "\t\tbatchImages = np.vstack(batchImages)\n",
    "\t\tfeatures = model.predict(batchImages, batch_size=BATCH_SIZE)\n",
    "\t\tfeatures = features.reshape((features.shape[0], 7 * 7 * 2048))\n",
    "\t\t# loop over the class labels and extracted features\n",
    "\t\tfor (label, vec) in zip(batchLabels, features):\n",
    "\t\t\t# construct a row that exists of the class label and\n",
    "\t\t\t# extracted features\n",
    "\t\t\tvec = \",\".join([str(v) for v in vec])\n",
    "\t\t\tcsv.write(\"{},{}\\n\".format(label, vec))\n",
    "\t# close the CSV file\n",
    "\tcsv.close()\n",
    "# serialize the label encoder to disk\n",
    "f = open(LE_PATH, \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecabf99",
   "metadata": {},
   "source": [
    "# Feature selection and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb882eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# error rate\n",
    "def error_rate(xtrain, ytrain, x, opts):\n",
    "    # parameters\n",
    "    k     = opts['k']\n",
    "    fold  = opts['fold']\n",
    "    xt    = fold['xt']\n",
    "    yt    = fold['yt']\n",
    "    xv    = fold['xv'] \n",
    "    yv    = fold['yv']\n",
    "    \n",
    "    # Number of instances\n",
    "    num_train = np.size(xt, 0)\n",
    "    num_valid = np.size(xv, 0)\n",
    "    # Define selected features\n",
    "    xtrain  = xt[:, x == 1]\n",
    "    ytrain  = yt.reshape(num_train)  # Solve bug\n",
    "    xvalid  = xv[:, x == 1]\n",
    "    yvalid  = yv.reshape(num_valid)  # Solve bug   \n",
    "    # Training\n",
    "    mdl     = KNeighborsClassifier(n_neighbors = k)\n",
    "    mdl.fit(xtrain, ytrain)\n",
    "    # Prediction\n",
    "    ypred   = mdl.predict(xvalid)\n",
    "    acc     = np.sum(yvalid == ypred) / num_valid\n",
    "    error   = 1 - acc\n",
    "    \n",
    "    return error\n",
    "\n",
    "\n",
    "# Error rate & Feature size\n",
    "def Fun(xtrain, ytrain, x, opts):\n",
    "    # Parameters\n",
    "    alpha    = 0.99\n",
    "    beta     = 1 - alpha\n",
    "    # Original feature size\n",
    "    max_feat = len(x)\n",
    "    # Number of selected features\n",
    "    num_feat = np.sum(x == 1)\n",
    "    # Solve if no feature selected\n",
    "    if num_feat == 0:\n",
    "        cost  = 1\n",
    "    else:\n",
    "        # Get error rate\n",
    "        error = error_rate(xtrain, ytrain, x, opts)\n",
    "        # Objective function\n",
    "        cost  = alpha * error + beta * (num_feat / max_feat)\n",
    "        \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "def init_position(lb, ub, N, dim):\n",
    "    X = np.zeros([N, dim], dtype='float')\n",
    "    for i in range(N):\n",
    "        for d in range(dim):\n",
    "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def init_velocity(lb, ub, N, dim):\n",
    "    V    = np.zeros([N, dim], dtype='float')\n",
    "    Vmax = np.zeros([1, dim], dtype='float')\n",
    "    Vmin = np.zeros([1, dim], dtype='float')\n",
    "    # Maximum & minimum velocity\n",
    "    for d in range(dim):\n",
    "        Vmax[0,d] = (ub[0,d] - lb[0,d]) / 2\n",
    "        Vmin[0,d] = -Vmax[0,d]\n",
    "        \n",
    "    for i in range(N):\n",
    "        for d in range(dim):\n",
    "            V[i,d] = Vmin[0,d] + (Vmax[0,d] - Vmin[0,d]) * rand()\n",
    "        \n",
    "    return V, Vmax, Vmin\n",
    "\n",
    "\n",
    "def binary_conversion(X, thres, N, dim):\n",
    "    Xbin = np.zeros([N, dim], dtype='int')\n",
    "    for i in range(N):\n",
    "        for d in range(dim):\n",
    "            if X[i,d] > thres:\n",
    "                Xbin[i,d] = 1\n",
    "            else:\n",
    "                Xbin[i,d] = 0\n",
    "    \n",
    "    return Xbin\n",
    "\n",
    "\n",
    "def boundary(x, lb, ub):\n",
    "    if x < lb:\n",
    "        x = lb\n",
    "    if x > ub:\n",
    "        x = ub\n",
    "    \n",
    "    return x\n",
    "    \n",
    "\n",
    "def jfs(xtrain, ytrain, opts):\n",
    "    # Parameters\n",
    "    ub    = 1\n",
    "    lb    = 0\n",
    "    thres = 0.5\n",
    "    w     = 0.9    # inertia weight\n",
    "    c1    = 2      # acceleration factor\n",
    "    c2    = 2      # acceleration factor\n",
    "    \n",
    "    N        = opts['N']\n",
    "    max_iter = opts['T']\n",
    "    if 'w' in opts:\n",
    "        w    = opts['w']\n",
    "    if 'c1' in opts:\n",
    "        c1   = opts['c1']\n",
    "    if 'c2' in opts:\n",
    "        c2   = opts['c2'] \n",
    "    \n",
    "    # Dimension\n",
    "    dim = np.size(xtrain, 1)\n",
    "    if np.size(lb) == 1:\n",
    "        ub = ub * np.ones([1, dim], dtype='float')\n",
    "        lb = lb * np.ones([1, dim], dtype='float')\n",
    "        \n",
    "    # Initialize position & velocity\n",
    "    X             = init_position(lb, ub, N, dim)\n",
    "    V, Vmax, Vmin = init_velocity(lb, ub, N, dim) \n",
    "    \n",
    "    # Pre\n",
    "    fit   = np.zeros([N, 1], dtype='float')\n",
    "    Xgb   = np.zeros([1, dim], dtype='float')\n",
    "    fitG  = float('inf')\n",
    "    Xpb   = np.zeros([N, dim], dtype='float')\n",
    "    fitP  = float('inf') * np.ones([N, 1], dtype='float')\n",
    "    curve = np.zeros([1, max_iter], dtype='float') \n",
    "    t     = 0\n",
    "    \n",
    "    while t < max_iter:\n",
    "        # Binary conversion\n",
    "        Xbin = binary_conversion(X, thres, N, dim)\n",
    "        \n",
    "        # Fitness\n",
    "        for i in range(N):\n",
    "            fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
    "            if fit[i,0] < fitP[i,0]:\n",
    "                Xpb[i,:]  = X[i,:]\n",
    "                fitP[i,0] = fit[i,0]\n",
    "            if fitP[i,0] < fitG:\n",
    "                Xgb[0,:]  = Xpb[i,:]\n",
    "                fitG      = fitP[i,0]\n",
    "        \n",
    "        # Store result\n",
    "        curve[0,t] = fitG.copy()\n",
    "        print(\"Iteration:\", t + 1)\n",
    "        print(\"Best (PSO):\", curve[0,t])\n",
    "        t += 1\n",
    "        \n",
    "        for i in range(N):\n",
    "            for d in range(dim):\n",
    "                # Update velocity\n",
    "                r1     = rand()\n",
    "                r2     = rand()\n",
    "                V[i,d] = w * V[i,d] + c1 * r1 * (Xpb[i,d] - X[i,d]) + c2 * r2 * (Xgb[0,d] - X[i,d]) \n",
    "                # Boundary\n",
    "                V[i,d] = boundary(V[i,d], Vmin[0,d], Vmax[0,d])\n",
    "                # Update position\n",
    "                X[i,d] = X[i,d] + V[i,d]\n",
    "                # Boundary\n",
    "                X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
    "    \n",
    "                \n",
    "    # Best feature subset\n",
    "    Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
    "    Gbin       = Gbin.reshape(dim)\n",
    "    pos        = np.asarray(range(0, dim))    \n",
    "    sel_index  = pos[Gbin == 1]\n",
    "    num_feat   = len(sel_index)\n",
    "    # Create dictionary\n",
    "    pso_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
    "    \n",
    "    return pso_data    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f55b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# load data\n",
    "data  = pd.read_csv('E:/PROJECT/split csv/training.csv')\n",
    "data  = data.values\n",
    "feat  = np.asarray(data[:, 1:-1])\n",
    "label = np.asarray(data[:, 0])\n",
    "\n",
    "# split data into train & validation 80 --20)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(feat, label, test_size=0.2, stratify=label)\n",
    "fold = {'xt':xtrain, 'yt':ytrain, 'xv':xtest, 'yv':ytest}\n",
    "\n",
    "# parameter\n",
    "k    = 5    # k-value in KNN\n",
    "N    = 10    # number of particles\n",
    "T    = 10   # maximum number of iterations\n",
    "opts = {'k':k, 'fold':fold, 'N':N, 'T':T}\n",
    "\n",
    "# perform feature selection\n",
    "fmdl = jfs(feat, label, opts)\n",
    "sf   = fmdl['sf']\n",
    "\n",
    "# model with selected features\n",
    "num_train = np.size(xtrain, 0)\n",
    "num_valid = np.size(xtest, 0)\n",
    "x_train   = xtrain[:, sf]\n",
    "y_train   = ytrain.reshape(num_train)  # Solve bug\n",
    "x_valid   = xtest[:, sf]\n",
    "y_valid   = ytest.reshape(num_valid)  # Solve bug\n",
    "\n",
    "# number of selected features\n",
    "num_feat = fmdl['nf']\n",
    "print(\"The number of features selected :\", num_feat)\n",
    "\n",
    "# plot convergence\n",
    "curve   = fmdl['c']\n",
    "curve   = curve.reshape(np.size(curve,1))\n",
    "x       = np.arange(0, opts['T'], 1.0) + 1.0\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, curve, 'o-')\n",
    "ax.set_xlabel('Number of Iterations')\n",
    "ax.set_ylabel('Fitness')\n",
    "ax.set_title('PSO')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ac162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix,precision_score, f1_score, recall_score,plot_confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# model with selected features\n",
    "num_train = np.size(xtrain, 0)\n",
    "num_valid = np.size(xtest, 0)\n",
    "x_train   = xtrain[:, sf]\n",
    "y_train   = ytrain.reshape(num_train)  # Solve bug\n",
    "x_valid   = xtest[:, sf]\n",
    "y_valid   = ytest.reshape(num_valid)  # Solve bug\n",
    "\n",
    "mdl = svm.SVC(kernel='linear')\n",
    "mdl.fit(x_train, y_train)\n",
    "\n",
    "# accuracy\n",
    "y_pred    = mdl.predict(x_valid)\n",
    "Acc= (np.sum(y_valid==y_pred) / num_valid)*100\n",
    "print(\"Accuracy : %.3f\" % Acc)\n",
    "\n",
    "Precision=precision_score(y_valid,y_pred)*100\n",
    "print(\"Precision : %.3f\" % Precision)\n",
    "\n",
    "Recall=recall_score(y_valid,y_pred)*100\n",
    "print(\"Recall : %.3f\" % Recall)\n",
    "\n",
    "F1=f1_score(y_valid,y_pred)*100\n",
    "print(\"F1-score : %.3f\" % F1)\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "f = sns.heatmap(cm, annot=True, fmt='d')\n",
    "total1=sum(sum(cm))\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])*100\n",
    "print('Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = cm[1,1]/(cm[1,0]+cm[1,1])*100\n",
    "print('Specificity : ', specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['Accuracy', 'Precision', 'Recall', 'F1_score','Sensitivity','Specificity']\n",
    "y= [Acc,Precision,Recall,F1,sensitivity,specificity]\n",
    "fig=plt.figure(figsize=(7,5))\n",
    "plt.ylim(80,100)\n",
    "plt.bar(x, y)\n",
    "plt.title('Quality analysis')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Percentange(%)')\n",
    "\n",
    "# Displaying the bar plot\n",
    "plt.show()\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2fe45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f614d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
